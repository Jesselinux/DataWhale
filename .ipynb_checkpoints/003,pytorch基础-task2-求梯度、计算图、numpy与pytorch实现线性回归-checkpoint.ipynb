{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1,计算图：PyTorch使用的是动态图，它的计算图在每次前向传播时都是从头开始构建\n",
    "# 使用retain_graph来保存buffer:\n",
    "from __future__ import print_function\n",
    "import torch as t\n",
    "\n",
    "\n",
    "x = t.ones(1)\n",
    "b = t.rand(1, requires_grad = True)   # 变量的requires_grad属性默认为False,节省约一半显存\n",
    "w = t.rand(1, requires_grad = True)\n",
    "y = w * x   # 等价于y=w.mul(x)\n",
    "z = y + b   # 等价于z=y.add(b)\n",
    "\n",
    "z.backward(retain_graph=True)\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2477,  3.8471,  0.1684,  7.0848],\n",
      "        [ 4.7594,  1.8777,  0.0249, 13.8255],\n",
      "        [ 4.0869, -0.3780,  3.5115, -0.0813]]) \n",
      " tensor([[-0.2477,  3.8471,  0.1684,  7.0848],\n",
      "        [ 4.7594,  1.8777,  0.0249, 13.8255],\n",
      "        [ 4.0869, -0.3780,  3.5115, -0.0813]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 2，autograd:\n",
    "def f(x):\n",
    "    '''计算y'''\n",
    "    y = x**2 * t.exp(x)\n",
    "    return y\n",
    "\n",
    "def gradf(x):\n",
    "    '''手动求导函数'''\n",
    "    dx = 2*x*t.exp(x) + x**2*t.exp(x)\n",
    "    return dx\n",
    "\n",
    "\n",
    "x = t.randn(3,4, requires_grad = True)\n",
    "y = f(x)\n",
    "\n",
    "y.backward(t.ones(y.size()))       # gradient形状与y一致\n",
    "print(x.grad, '\\n', gradf(x))      # autograd的计算结果与利用公式手动计算的结果一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1.2777559757232666\n",
      "epoch: 1, loss: 0.23446236550807953\n",
      "epoch: 2, loss: 0.21228225529193878\n",
      "epoch: 3, loss: 0.21166886389255524\n",
      "epoch: 4, loss: 0.21151168644428253\n",
      "epoch: 5, loss: 0.21136493980884552\n",
      "epoch: 6, loss: 0.21121908724308014\n",
      "epoch: 7, loss: 0.21107405424118042\n",
      "epoch: 8, loss: 0.2109297215938568\n",
      "epoch: 9, loss: 0.21078616380691528\n",
      "epoch: 10, loss: 0.21064333617687225\n",
      "epoch: 11, loss: 0.21050123870372772\n",
      "epoch: 12, loss: 0.21035988628864288\n",
      "epoch: 13, loss: 0.21021929383277893\n",
      "epoch: 14, loss: 0.2100793570280075\n",
      "epoch: 15, loss: 0.20994016528129578\n",
      "epoch: 16, loss: 0.20980171859264374\n",
      "epoch: 17, loss: 0.20966394245624542\n",
      "epoch: 18, loss: 0.2095268815755844\n",
      "epoch: 19, loss: 0.2093905806541443\n",
      "epoch: 20, loss: 0.20925498008728027\n",
      "epoch: 21, loss: 0.2091200351715088\n",
      "epoch: 22, loss: 0.2089858055114746\n",
      "epoch: 23, loss: 0.20885224640369415\n",
      "epoch: 24, loss: 0.20871946215629578\n",
      "epoch: 25, loss: 0.20858722925186157\n",
      "epoch: 26, loss: 0.20845580101013184\n",
      "epoch: 27, loss: 0.20832496881484985\n",
      "epoch: 28, loss: 0.2081948220729828\n",
      "epoch: 29, loss: 0.20806542038917542\n",
      "epoch: 30, loss: 0.2079365998506546\n",
      "epoch: 31, loss: 0.2078084796667099\n",
      "epoch: 32, loss: 0.20768095552921295\n",
      "epoch: 33, loss: 0.2075541615486145\n",
      "epoch: 34, loss: 0.20742802321910858\n",
      "epoch: 35, loss: 0.2073025405406952\n",
      "epoch: 36, loss: 0.20717766880989075\n",
      "epoch: 37, loss: 0.20705349743366241\n",
      "epoch: 38, loss: 0.20692990720272064\n",
      "epoch: 39, loss: 0.2068069875240326\n",
      "epoch: 40, loss: 0.2066846340894699\n",
      "epoch: 41, loss: 0.20656299591064453\n",
      "epoch: 42, loss: 0.2064419686794281\n",
      "epoch: 43, loss: 0.20632152259349823\n",
      "epoch: 44, loss: 0.20620177686214447\n",
      "epoch: 45, loss: 0.2060825228691101\n",
      "epoch: 46, loss: 0.20596401393413544\n",
      "epoch: 47, loss: 0.20584610104560852\n",
      "epoch: 48, loss: 0.20572873950004578\n",
      "epoch: 49, loss: 0.2056119590997696\n",
      "epoch: 50, loss: 0.20549587905406952\n",
      "epoch: 51, loss: 0.2053803652524948\n",
      "epoch: 52, loss: 0.20526541769504547\n",
      "epoch: 53, loss: 0.2051510512828827\n",
      "epoch: 54, loss: 0.20503726601600647\n",
      "epoch: 55, loss: 0.20492412149906158\n",
      "epoch: 56, loss: 0.20481155812740326\n",
      "epoch: 57, loss: 0.2046995311975479\n",
      "epoch: 58, loss: 0.2045881450176239\n",
      "epoch: 59, loss: 0.2044772505760193\n",
      "epoch: 60, loss: 0.20436695218086243\n",
      "epoch: 61, loss: 0.2042572796344757\n",
      "epoch: 62, loss: 0.20414815843105316\n",
      "epoch: 63, loss: 0.2040395587682724\n",
      "epoch: 64, loss: 0.20393157005310059\n",
      "epoch: 65, loss: 0.20382405817508698\n",
      "epoch: 66, loss: 0.2037171721458435\n",
      "epoch: 67, loss: 0.20361079275608063\n",
      "epoch: 68, loss: 0.2035049945116043\n",
      "epoch: 69, loss: 0.20339970290660858\n",
      "epoch: 70, loss: 0.20329497754573822\n",
      "epoch: 71, loss: 0.20319083333015442\n",
      "epoch: 72, loss: 0.20308715105056763\n",
      "epoch: 73, loss: 0.20298407971858978\n",
      "epoch: 74, loss: 0.20288150012493134\n",
      "epoch: 75, loss: 0.20277945697307587\n",
      "epoch: 76, loss: 0.20267793536186218\n",
      "epoch: 77, loss: 0.20257696509361267\n",
      "epoch: 78, loss: 0.20247647166252136\n",
      "epoch: 79, loss: 0.20237646996974945\n",
      "epoch: 80, loss: 0.2022770792245865\n",
      "epoch: 81, loss: 0.20217815041542053\n",
      "epoch: 82, loss: 0.20207974314689636\n",
      "epoch: 83, loss: 0.201981782913208\n",
      "epoch: 84, loss: 0.20188437402248383\n",
      "epoch: 85, loss: 0.20178750157356262\n",
      "epoch: 86, loss: 0.20169109106063843\n",
      "epoch: 87, loss: 0.20159517228603363\n",
      "epoch: 88, loss: 0.201499804854393\n",
      "epoch: 89, loss: 0.2014048993587494\n",
      "epoch: 90, loss: 0.2013104259967804\n",
      "epoch: 91, loss: 0.20121650397777557\n",
      "epoch: 92, loss: 0.20112305879592896\n",
      "epoch: 93, loss: 0.20103012025356293\n",
      "epoch: 94, loss: 0.20093759894371033\n",
      "epoch: 95, loss: 0.20084555447101593\n",
      "epoch: 96, loss: 0.2007540613412857\n",
      "epoch: 97, loss: 0.20066294074058533\n",
      "epoch: 98, loss: 0.20057234168052673\n",
      "epoch: 99, loss: 0.20048226416110992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f80b2708240>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3，numpy、pytorch进行梯度优化一个线性模型：\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "torch.manual_seed(2018)\n",
    "\n",
    "x_train = np.array([[3.3],[4.4],[5.5],[6.17],[6.93],[4.168],[9.779],[6.182],[7.59],[2.167],[7.042],[10.791],\n",
    "                    [5.313],[7.997],[3.1]],dtype=np.float32)\n",
    "y_train = np.array([[1.7],[2.76],[2.09],[3.19],[1.694],[1.573],[3.366],[2.596],[2.53],[1.221],[2.827],\n",
    "                   [3.465],[1.65],[2.904],[1.3]],dtype=np.float32)\n",
    "\n",
    "plt.plot(x_train, y_train, 'bo')\n",
    "\n",
    "# numpy转化为torch，以便梯度优化线性模型：\n",
    "pt_x_train = torch.from_numpy(x_train)\n",
    "pt_y_train = torch.from_numpy(y_train)\n",
    "\n",
    "w = Variable(torch.randn(1), requires_grad=True)\n",
    "b = Variable(torch.randn(1), requires_grad=True)\n",
    "\n",
    "x = Variable(pt_x_train)\n",
    "y = Variable(pt_y_train)\n",
    "\n",
    "def linear_model(x):  # 定义线性模型\n",
    "    return x * w + b\n",
    "\n",
    "y_hat = linear_model(x)\n",
    "\n",
    "# plt.plot(x.data.numpy(), y.data.numpy(), 'bo', label='real')\n",
    "# plt.plot(x.data.numpy(), y_hat.data.numpy(), 'ro', label='estimated')\n",
    "# plt.legend()\n",
    "\n",
    "def get_loss(y_hat, y):  # 定义损失函数，梯度优化的目标是最小化损失函数\n",
    "    return torch.mean((y_hat - y) ** 2)\n",
    "\n",
    "loss = get_loss(y_hat, y)\n",
    "\n",
    "loss.backward()\n",
    "w.data = w.data - 1e-2 * w.grad.data  # 梯度优化，进行参数更新\n",
    "b.data = b.data - 1e-2 * b.grad.data\n",
    "\n",
    "# y_hat = linear_model(x)\n",
    "# plt.plot(x.data.numpy(), y.data.numpy(), 'bo', label='real')\n",
    "# plt.plot(x.data.numpy(), y_hat.data.numpy(), 'ro', label='estimated')\n",
    "# plt.legend()\n",
    "\n",
    "for e in range(100):  # 参数更新100次\n",
    "    y_hat = linear_model(x)\n",
    "    loss = get_loss(y_hat, y)\n",
    "    \n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    loss.backward()\n",
    "    \n",
    "    w.data = w.data - 1e-2 * w.grad.data\n",
    "    b.data = b.data - 1e-2 * b.grad.data \n",
    "    print('epoch: {}, loss: {}'.format(e, loss.item()))\n",
    "    \n",
    "y_hat = linear_model(x)  # 画图查看拟合效果\n",
    "plt.plot(x.data.numpy(), y.data.numpy(), 'bo', label='real')\n",
    "plt.plot(x.data.numpy(), y_hat.data.numpy(), 'ro', label='estimated')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2795e-01, -4.6099e-01, -3.8386e-01, -3.4965e-01],\n",
      "        [-4.0535e-01, -2.3546e-01, -2.9070e-01, -1.2966e-03],\n",
      "        [-4.2821e-01, -4.4423e-01, -4.4904e-01,  9.3193e+01]]) \n",
      " tensor([[-2.2795e-01, -4.6099e-01, -3.8386e-01, -3.4965e-01],\n",
      "        [-4.0535e-01, -2.3546e-01, -2.9070e-01, -1.2966e-03],\n",
      "        [-4.2821e-01, -4.4423e-01, -4.4904e-01,  9.3193e+01]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
