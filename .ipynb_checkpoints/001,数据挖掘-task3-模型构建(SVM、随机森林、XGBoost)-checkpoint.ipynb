{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 7\n",
      "(4754, 90)\n",
      "44 90\n",
      "{'2', '4', '3', '0', '1'}\n",
      " 1) low_volume_percent             0.050628\n",
      " 2) middle_volume_percent          0.049992\n",
      " 3) take_amount_in_later_12_month_highest 0.033458\n",
      " 4) trans_amount_increase_rate_lately 0.031082\n",
      " 5) trans_activity_month           0.029079\n",
      " 6) trans_activity_day             0.024676\n",
      " 7) transd_mcc                     0.021055\n",
      " 8) trans_days_interval_filter     0.018402\n",
      " 9) trans_days_interval            0.018395\n",
      "10) regional_mobility              0.017821\n",
      "11) repayment_capability           0.017615\n",
      "12) is_high_user                   0.016818\n",
      "13) number_of_trans_from_2011      0.016479\n",
      "14) first_transaction_time         0.016430\n",
      "15) historical_trans_amount        0.016297\n",
      "16) historical_trans_day           0.015933\n",
      "17) rank_trad_1_month              0.015679\n",
      "18) trans_amount_3_month           0.014631\n",
      "19) avg_consume_less_12_valid_month 0.014053\n",
      "20) abs                            0.013458\n",
      "21) top_trans_count_last_1_month   0.012876\n",
      "22) avg_price_last_12_month        0.012760\n",
      "23) avg_price_top_last_12_valid_month 0.012693\n",
      "24) trans_top_time_last_1_month    0.012688\n",
      "25) trans_top_time_last_6_month    0.012645\n",
      "26) consume_top_time_last_1_month  0.012607\n",
      "27) consume_top_time_last_6_month  0.012351\n",
      "28) cross_consume_count_last_1_month 0.012262\n",
      "29) trans_fail_top_count_enum_last_1_month 0.012259\n",
      "30) trans_fail_top_count_enum_last_6_month 0.011929\n",
      "31) trans_fail_top_count_enum_last_12_month 0.011621\n",
      "32) consume_mini_time_last_1_month 0.011576\n",
      "33) max_cumulative_consume_later_1_month 0.011550\n",
      "34) max_consume_count_later_6_month 0.011528\n",
      "35) railway_consume_count_last_12_month 0.011490\n",
      "36) pawns_auctions_trusts_consume_last_1_month 0.011487\n",
      "37) pawns_auctions_trusts_consume_last_6_month 0.011434\n",
      "38) jewelry_consume_count_last_6_month 0.011315\n",
      "39) first_transaction_day          0.010912\n",
      "40) trans_day_last_12_month        0.010712\n",
      "41) apply_score                    0.010710\n",
      "42) apply_credibility              0.010663\n",
      "43) query_org_count                0.010363\n",
      "44) query_finance_count            0.010244\n",
      "45) query_cash_count               0.010188\n",
      "46) query_sum_count                0.010143\n",
      "47) latest_one_month_apply         0.010097\n",
      "48) latest_three_month_apply       0.010031\n",
      "49) latest_six_month_apply         0.010010\n",
      "50) loans_score                    0.009934\n",
      "51) loans_credibility_behavior     0.009735\n",
      "52) loans_count                    0.009508\n",
      "53) loans_settle_count             0.009355\n",
      "54) loans_overdue_count            0.009083\n",
      "55) loans_org_count_behavior       0.009078\n",
      "56) consfin_org_count_behavior     0.009027\n",
      "57) loans_cash_count               0.008988\n",
      "58) latest_one_month_loan          0.008984\n",
      "59) latest_three_month_loan        0.008596\n",
      "60) latest_six_month_loan          0.008517\n",
      "61) history_suc_fee                0.008505\n",
      "62) history_fail_fee               0.008426\n",
      "63) latest_one_month_suc           0.008423\n",
      "64) latest_one_month_fail          0.008405\n",
      "65) loans_long_time                0.008288\n",
      "66) loans_credit_limit             0.008148\n",
      "67) loans_credibility_limit        0.007914\n",
      "68) loans_org_count_current        0.007715\n",
      "69) loans_product_count            0.007144\n",
      "70) loans_max_limit                0.006932\n",
      "71) loans_avg_limit                0.006869\n",
      "72) consfin_credit_limit           0.006645\n",
      "73) consfin_credibility            0.005776\n",
      "74) consfin_org_count_current      0.005644\n",
      "75) consfin_product_count          0.005596\n",
      "76) consfin_max_limit              0.004546\n",
      "77) consfin_avg_limit              0.000587\n",
      "78) latest_query_day               0.000446\n",
      "79) loans_latest_day               0.000060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 0，task1 and task2:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection  import train_test_split\n",
    "\n",
    "\n",
    "data = pd.read_csv('../DataSets/001data.csv', encoding='gbk')\n",
    "\n",
    "columns_dropna = ['bank_card_no', 'reg_preference_for_trad', 'id_name','first_transaction_time','trade_no',\n",
    "                 'latest_query_time','loans_latest_time']  # 对于这些列实行删除NA样本的操作\n",
    "temp_columns = list(data.columns.values)\n",
    "columns_imputer = []                                       # 对于剩余的列实行填充0值的操作\n",
    "for i in range(len(temp_columns)):\n",
    "    if temp_columns[i] not in columns_dropna:\n",
    "        columns_imputer.append(temp_columns[i])\n",
    "print(len(columns_imputer), len(columns_dropna))\n",
    "\n",
    "print(data.shape)\n",
    "data.dropna(axis=0, how='any', subset=columns_dropna, inplace=True)         # 删除缺省值样本\n",
    "data = data.replace(np.NaN, 0)                                              # 缺省值填充0\n",
    "\n",
    "column_headers = list(data.columns.values)\n",
    "print(column_headers.index('status'), len(column_headers))\n",
    "\n",
    "\n",
    "# 删掉id数据：\n",
    "columns_drop = ['bank_card_no','source','id_name','custid','student_feature','trade_no']\n",
    "data.drop(columns_drop, axis=1, inplace=True)\n",
    "# 日期数据格式化：\n",
    "data['latest_query_time'] = pd.to_datetime(data['latest_query_time'])\n",
    "data['loans_latest_time'] = pd.to_datetime(data['loans_latest_time'])\n",
    "# 将城市特征数值化：\n",
    "map_dic = {'一线城市':'1','二线城市':'2','三线城市':'3','其他城市':'4','境外':'0'}\n",
    "data['reg_preference_for_trad'] = data['reg_preference_for_trad'].map(map_dic)\n",
    "# 将时间戳数据列丢掉：\n",
    "data.drop(['latest_query_time','loans_latest_time'], axis=1, inplace=True)\n",
    "# # 丢掉方差为零的列：\n",
    "# data.drop(data.columns[data.std()==0], axis=1, inplace=True)\n",
    "data.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "#print(data.columns.values)\n",
    "print(set(data['reg_preference_for_trad']))\n",
    "\n",
    "# 统计缺失值占比：\n",
    "data_missing = (data.isnull().sum()/len(data))*100    # np.isnan(data):可将他替换成 data.isnull()\n",
    "data_missing = data_missing.drop(data_missing[data_missing==0].index).sort_values(ascending=False)\n",
    "miss_data = pd.DataFrame({'缺失百分比':data_missing})\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier   # 使用随机会森林进行特征选择\n",
    "x_columns = []  # X的列属性(x变量)\n",
    "for i in range(len(column_headers)):\n",
    "    if column_headers[i] not in ['Unnamed: 0','reg_preference_for_trad','latest_query_time','loans_latest_time','status','bank_card_no','source','id_name','unnameid','custid','student_feature','trade_no']:\n",
    "        x_columns.append(column_headers[i])  \n",
    "        \n",
    "X = data[x_columns]  # 获取x变量\n",
    "Y = data['status']   # 获取y标签(label)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X, Y)\n",
    "\n",
    "importance = clf.feature_importances_\n",
    "indices = np.argsort(importance)[::-1]\n",
    "features = X.columns\n",
    "for f in range(X.shape[1]):\n",
    "    print((\"%2d) %-*s %f\" % (f + 1, 30, features[f], importance[indices[f]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4429, 47)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 特征工程：\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X_fsvar = VarianceThreshold().fit_transform(X) # 过滤掉方差为零的列，并实例化，不填写参数默认方差为0\n",
    "nan_rate = pd.DataFrame((data.shape[0]-data.count())/data.shape[0])  #查看缺失值比例\n",
    "nan_rate\n",
    "\n",
    "# 特征工程：互信息法,它返回每个特征与目标之间的互信息量的估计，这个估计在[0,1]之间取值，0表示独立，1表示两个变量完全相关\n",
    "from sklearn.feature_selection import mutual_info_classif as MIC\n",
    "result = MIC(X,Y)\n",
    "print((result == 0).sum())\n",
    "delete = []\n",
    "for i in range(79):\n",
    "    if result[i] == 0:\n",
    "        delete.append(i)\n",
    "X_ = X.drop(X.iloc[:,delete],axis=1)\n",
    "X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3360482654600302\n",
      "样本个数：6630; 1占50.00%; 0占50.00%\n"
     ]
    }
   ],
   "source": [
    "# SMOTE:样本不均衡\n",
    "print(Y[Y==1].count()/Y[Y==0].count())\n",
    "\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=420)\n",
    "X,Y = sm.fit_sample(X_,Y)\n",
    "\n",
    "n_sample_ = X.shape[0]\n",
    "\n",
    "pd.Series(Y).value_counts()\n",
    "\n",
    "n_1_sample = pd.Series(Y).value_counts()[1]\n",
    "n_0_sample = pd.Series(Y).value_counts()[0]\n",
    "print('样本个数：{}; 1占{:.2%}; 0占{:.2%}'.format(n_sample_,n_1_sample/n_sample_,n_0_sample/n_sample_))\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率:0.7677,精确率:0.7753,召回率:0.7550,f1-score:0.7650,auc:0.7677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/home/jesse/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 逻辑回归模型：\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7, random_state=2018)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "lr = LR().fit(x_train, y_train)\n",
    "#lr.score(Xtest,Ytest)\n",
    "from sklearn import metrics\n",
    "y_test_pred = lr.predict(x_test)\n",
    "acc = metrics.accuracy_score(y_test,y_test_pred)\n",
    "precision = metrics.precision_score(y_test,y_test_pred)\n",
    "recall = metrics.recall_score(y_test,y_test_pred)\n",
    "f1 = metrics.f1_score(y_test,y_test_pred)\n",
    "auc= metrics.roc_auc_score(y_test,y_test_pred)\n",
    "print('准确率:{:.4f},精确率:{:.4f},召回率:{:.4f},f1-score:{:.4f},auc:{:.4f}'.format(acc,precision,recall,f1,auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率:0.8074,精确率:0.8032,召回率:0.8153,f1-score:0.8092,auc:0.8074\n"
     ]
    }
   ],
   "source": [
    "# SVM:\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='rbf',gamma='auto',cache_size=5000).fit(x_train, y_train)\n",
    "clf.score(x_test, y_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "y_test_pred = clf.predict(x_test)\n",
    "acc = metrics.accuracy_score(y_test,y_test_pred)\n",
    "precision = metrics.precision_score(y_test,y_test_pred)\n",
    "recall = metrics.recall_score(y_test,y_test_pred)\n",
    "f1 = metrics.f1_score(y_test,y_test_pred)\n",
    "auc= metrics.roc_auc_score(y_test,y_test_pred)\n",
    "print('准确率:{:.4f},精确率:{:.4f},召回率:{:.4f},f1-score:{:.4f},auc:{:.4f}'.format(acc,precision,recall,f1,auc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率:0.8074,精确率:0.8032,召回率:0.8153,f1-score:0.8092,auc:0.8074\n"
     ]
    }
   ],
   "source": [
    "# 随机森林：\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "rfc.fit(x_train,y_train)\n",
    "rfc.score(x_test,y_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "y_test_pred = clf.predict(x_test)\n",
    "acc = metrics.accuracy_score(y_test,y_test_pred)\n",
    "precision = metrics.precision_score(y_test,y_test_pred)\n",
    "recall = metrics.recall_score(y_test,y_test_pred)\n",
    "f1 = metrics.f1_score(y_test,y_test_pred)\n",
    "auc= metrics.roc_auc_score(y_test,y_test_pred)\n",
    "print('准确率:{:.4f},精确率:{:.4f},召回率:{:.4f},f1-score:{:.4f},auc:{:.4f}'.format(acc,precision,recall,f1,auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率:0.8532,精确率:0.8777,召回率:0.8213,f1-score:0.8485,auc:0.8532\n"
     ]
    }
   ],
   "source": [
    "# XGBoost:\n",
    "from xgboost.sklearn import XGBClassifier as XGBC\n",
    "clf = XGBC().fit(x_train,y_train)\n",
    "clf.score(x_test,y_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "y_test_pred = clf.predict(x_test)\n",
    "acc = metrics.accuracy_score(y_test,y_test_pred)\n",
    "precision = metrics.precision_score(y_test,y_test_pred)\n",
    "recall = metrics.recall_score(y_test,y_test_pred)\n",
    "f1 = metrics.f1_score(y_test,y_test_pred)\n",
    "auc= metrics.roc_auc_score(y_test,y_test_pred)\n",
    "print('准确率:{:.4f},精确率:{:.4f},召回率:{:.4f},f1-score:{:.4f},auc:{:.4f}'.format(acc,precision,recall,f1,auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
