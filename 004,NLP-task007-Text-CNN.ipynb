{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''词向量模型：\n",
    "\n",
    "　　　　一般开始为高纬度，高稀疏向量，利用嵌入层对其进行降维，增加稠密性。\n",
    "\n",
    "　　　　使用词向量进行文本分类的步骤为：\n",
    "\n",
    "　　　　　　①.先使用分词工具提取词汇表。\n",
    "\n",
    "　　　　　　②.将要分类的内容转换为词向量。\n",
    "\n",
    "　　　　　　　　a.分词\n",
    "\n",
    "　　　　　　　　b.将每个词转换为word2vec向量。\n",
    "\n",
    "　　　　　　　　c.按顺序组合word2vec，那么就组合成了一个词向量。\n",
    "\n",
    "　　　　　　　　d.卷积、池化和连接，然后进行分类。\n",
    "        https://blog.csdn.net/randompeople/article/details/90553665'''\n",
    "\n",
    "\n",
    "import pickle\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolution, max-pooling and soft-max layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.lr = config['lr']\n",
    "        self.batch_size = config['batch_size']\n",
    "        # 词典的大小\n",
    "        self.vocab_size = config['vocab_size']\n",
    "        self.num_classes = config['num_classes']\n",
    "        self.keep_prob = config['keep_prob']\n",
    "        # length of word embedding\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        # seting filter sizes, type of list\n",
    "        self.filter_sizes = config['filter_sizes']\n",
    "        # max length of sentence\n",
    "        self.sentence_length = config['sentence_length']\n",
    "        # number of filters\n",
    "        self.num_filters = config['num_filters']\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        self.X = tf.placeholder('int32', [None, self.sentence_length])\n",
    "        self.y = tf.placeholder('int32', [None, ])\n",
    "\n",
    "    def inference(self):\n",
    "        with tf.variable_scope('embedding_layer'):\n",
    "            # loading embedding weights\n",
    "            with open('Text_cnn/embedding_matrix.pkl','rb') as f:\n",
    "                embedding_weights = pickle.load(f)\n",
    "            # non-static \n",
    "            self.W = tf.Variable(embedding_weights, trainable=True, name='embedding_weights',dtype='float32')\n",
    "            # shape of embedding chars is (None, sentence_length, embedding_size)\n",
    "            self.embedding_chars = tf.nn.embedding_lookup(self.W, self.X)\n",
    "            # shape of embedding char expanded is (None, sentence_length, embedding_size, 1)\n",
    "            self.embedding_chars_expanded = tf.expand_dims(self.embedding_chars, -1)\n",
    "        with tf.variable_scope('convolution_pooling_layer'):\n",
    "            pooled_outputs = []\n",
    "            for i, filter_size in enumerate(self.filter_sizes):\n",
    "                filter_shape = [filter_size, self.embedding_size, 1, self.num_filters]\n",
    "                W = tf.get_variable('W'+str(i), shape=filter_shape,\n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "                b = tf.get_variable('b'+str(i), shape=[self.num_filters],\n",
    "                                    initializer=tf.zeros_initializer())\n",
    "                conv = tf.nn.conv2d(self.embedding_chars_expanded, W, strides=[1,1,1,1],\n",
    "                                    padding='VALID', name='conv'+str(i))\n",
    "                # apply nonlinearity\n",
    "                h = tf.nn.relu(tf.add(conv, b))\n",
    "                # max pooling\n",
    "                pooled = tf.nn.max_pool(h, ksize=[1, self.sentence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1], padding='VALID', name=\"pool\")\n",
    "                # shape of pooled is (?,1,1,300)\n",
    "                pooled_outputs.append(pooled)\n",
    "            # combine all the pooled features\n",
    "            self.feature_length = self.num_filters * len(self.filter_sizes)\n",
    "            self.h_pool = tf.concat(pooled_outputs,3)\n",
    "            # shape of (?, 900)\n",
    "            self.h_pool_flat = tf.reshape(self.h_pool, [-1, self.feature_length])\n",
    "        # add dropout before softmax layer\n",
    "        with tf.variable_scope('dropout_layer'):\n",
    "            # shape of [None, feature_length]\n",
    "            self.features = tf.nn.dropout(self.h_pool_flat, self.keep_prob)\n",
    "        # fully-connection layer\n",
    "        with tf.variable_scope('fully_connection_layer'):\n",
    "            W = tf.get_variable('W', shape=[self.feature_length, self.num_classes],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.get_variable('b', shape=[self.num_classes],\n",
    "                                initializer=tf.constant_initializer(0.1))\n",
    "            # shape of [None, 2]\n",
    "            self.y_out = tf.matmul(self.features, W) + b\n",
    "            self.y_prob = tf.nn.softmax(self.y_out)\n",
    "\n",
    "    def add_loss(self):\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits=self.y_out)\n",
    "        self.loss = tf.reduce_mean(loss)\n",
    "        tf.summary.scalar('loss',self.loss)\n",
    "\n",
    "    def add_metric(self):\n",
    "        self.y_pred = self.y_prob[:,1] > 0.5\n",
    "        self.precision, self.precision_op = tf.metrics.precision(self.y, self.y_pred)\n",
    "        self.recall, self.recall_op = tf.metrics.recall(self.y, self.y_pred)\n",
    "        # add precision and recall to summary\n",
    "        tf.summary.scalar('precision', self.precision)\n",
    "        tf.summary.scalar('recall', self.recall)\n",
    "\n",
    "    def train(self):\n",
    "        # Applies exponential decay to learning rate\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        # define optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            self.train_op = optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\"build graph for model\"\"\"\n",
    "        self.add_placeholders()\n",
    "        self.inference()\n",
    "        self.add_loss()\n",
    "        self.add_metric()\n",
    "        self.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, val, test), batch_sizes=(32, 256, 256),shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
