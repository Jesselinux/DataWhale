{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 7\n",
      "(4754, 90)\n",
      "44 90\n",
      "{'0', '2', '1', '3', '4'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) low_volume_percent             0.056326\n",
      " 2) middle_volume_percent          0.040843\n",
      " 3) take_amount_in_later_12_month_highest 0.035887\n",
      " 4) trans_amount_increase_rate_lately 0.032025\n",
      " 5) trans_activity_month           0.027848\n",
      " 6) trans_activity_day             0.023640\n",
      " 7) transd_mcc                     0.018634\n",
      " 8) trans_days_interval_filter     0.017504\n",
      " 9) trans_days_interval            0.017166\n",
      "10) regional_mobility              0.016567\n",
      "11) repayment_capability           0.016010\n",
      "12) is_high_user                   0.015604\n",
      "13) number_of_trans_from_2011      0.015309\n",
      "14) first_transaction_time         0.015246\n",
      "15) historical_trans_amount        0.015173\n",
      "16) historical_trans_day           0.015078\n",
      "17) rank_trad_1_month              0.015061\n",
      "18) trans_amount_3_month           0.014939\n",
      "19) avg_consume_less_12_valid_month 0.014154\n",
      "20) abs                            0.014035\n",
      "21) top_trans_count_last_1_month   0.013946\n",
      "22) avg_price_last_12_month        0.013825\n",
      "23) avg_price_top_last_12_valid_month 0.013755\n",
      "24) trans_top_time_last_1_month    0.013551\n",
      "25) trans_top_time_last_6_month    0.013500\n",
      "26) consume_top_time_last_1_month  0.013049\n",
      "27) consume_top_time_last_6_month  0.013044\n",
      "28) cross_consume_count_last_1_month 0.013006\n",
      "29) trans_fail_top_count_enum_last_1_month 0.012980\n",
      "30) trans_fail_top_count_enum_last_6_month 0.012839\n",
      "31) trans_fail_top_count_enum_last_12_month 0.012466\n",
      "32) consume_mini_time_last_1_month 0.012230\n",
      "33) max_cumulative_consume_later_1_month 0.012193\n",
      "34) max_consume_count_later_6_month 0.012188\n",
      "35) railway_consume_count_last_12_month 0.012174\n",
      "36) pawns_auctions_trusts_consume_last_1_month 0.012154\n",
      "37) pawns_auctions_trusts_consume_last_6_month 0.011879\n",
      "38) jewelry_consume_count_last_6_month 0.011841\n",
      "39) first_transaction_day          0.011772\n",
      "40) trans_day_last_12_month        0.011411\n",
      "41) apply_score                    0.011177\n",
      "42) apply_credibility              0.011160\n",
      "43) query_org_count                0.011125\n",
      "44) query_finance_count            0.010830\n",
      "45) query_cash_count               0.010774\n",
      "46) query_sum_count                0.010772\n",
      "47) latest_one_month_apply         0.010619\n",
      "48) latest_three_month_apply       0.010521\n",
      "49) latest_six_month_apply         0.010347\n",
      "50) loans_score                    0.010118\n",
      "51) loans_credibility_behavior     0.009881\n",
      "52) loans_count                    0.009683\n",
      "53) loans_settle_count             0.009657\n",
      "54) loans_overdue_count            0.009594\n",
      "55) loans_org_count_behavior       0.009405\n",
      "56) consfin_org_count_behavior     0.008978\n",
      "57) loans_cash_count               0.008873\n",
      "58) latest_one_month_loan          0.008666\n",
      "59) latest_three_month_loan        0.008313\n",
      "60) latest_six_month_loan          0.008249\n",
      "61) history_suc_fee                0.008221\n",
      "62) history_fail_fee               0.007876\n",
      "63) latest_one_month_suc           0.007686\n",
      "64) latest_one_month_fail          0.007612\n",
      "65) loans_long_time                0.007603\n",
      "66) loans_credit_limit             0.007586\n",
      "67) loans_credibility_limit        0.007569\n",
      "68) loans_org_count_current        0.007530\n",
      "69) loans_product_count            0.007424\n",
      "70) loans_max_limit                0.007405\n",
      "71) loans_avg_limit                0.007287\n",
      "72) consfin_credit_limit           0.006886\n",
      "73) consfin_credibility            0.006703\n",
      "74) consfin_org_count_current      0.004573\n",
      "75) consfin_product_count          0.003845\n",
      "76) consfin_max_limit              0.003023\n",
      "77) consfin_avg_limit              0.001473\n",
      "78) latest_query_day               0.000106\n",
      "79) loans_latest_day               0.000000\n",
      "32\n",
      "0.3360482654600302\n",
      "样本个数：6630; 1占50.00%; 0占50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 0，task1、task2 and task3:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection  import train_test_split\n",
    "\n",
    "\n",
    "data = pd.read_csv('../DataSets/001data.csv', encoding='gbk')\n",
    "\n",
    "columns_dropna = ['bank_card_no', 'reg_preference_for_trad', 'id_name','first_transaction_time','trade_no',\n",
    "                 'latest_query_time','loans_latest_time']  # 对于这些列实行删除NA样本的操作\n",
    "temp_columns = list(data.columns.values)\n",
    "columns_imputer = []                                       # 对于剩余的列实行填充0值的操作\n",
    "for i in range(len(temp_columns)):\n",
    "    if temp_columns[i] not in columns_dropna:\n",
    "        columns_imputer.append(temp_columns[i])\n",
    "print(len(columns_imputer), len(columns_dropna))\n",
    "\n",
    "print(data.shape)\n",
    "data.dropna(axis=0, how='any', subset=columns_dropna, inplace=True)         # 删除缺省值样本\n",
    "data = data.replace(np.NaN, 0)                                              # 缺省值填充0\n",
    "\n",
    "column_headers = list(data.columns.values)\n",
    "print(column_headers.index('status'), len(column_headers))\n",
    "\n",
    "\n",
    "# 删掉id数据：\n",
    "columns_drop = ['bank_card_no','source','id_name','custid','student_feature','trade_no']\n",
    "data.drop(columns_drop, axis=1, inplace=True)\n",
    "# 日期数据格式化：\n",
    "data['latest_query_time'] = pd.to_datetime(data['latest_query_time'])\n",
    "data['loans_latest_time'] = pd.to_datetime(data['loans_latest_time'])\n",
    "# 将城市特征数值化：\n",
    "map_dic = {'一线城市':'1','二线城市':'2','三线城市':'3','其他城市':'4','境外':'0'}\n",
    "data['reg_preference_for_trad'] = data['reg_preference_for_trad'].map(map_dic)\n",
    "# 将时间戳数据列丢掉：\n",
    "data.drop(['latest_query_time','loans_latest_time'], axis=1, inplace=True)\n",
    "# # 丢掉方差为零的列：\n",
    "# data.drop(data.columns[data.std()==0], axis=1, inplace=True)\n",
    "data.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "#print(data.columns.values)\n",
    "print(set(data['reg_preference_for_trad']))\n",
    "\n",
    "# 统计缺失值占比：\n",
    "data_missing = (data.isnull().sum()/len(data))*100    # np.isnan(data):可将他替换成 data.isnull()\n",
    "data_missing = data_missing.drop(data_missing[data_missing==0].index).sort_values(ascending=False)\n",
    "miss_data = pd.DataFrame({'缺失百分比':data_missing})\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier   # 使用随机会森林进行特征选择\n",
    "x_columns = []  # X的列属性(x变量)\n",
    "for i in range(len(column_headers)):\n",
    "    if column_headers[i] not in ['Unnamed: 0','reg_preference_for_trad','latest_query_time','loans_latest_time','status','bank_card_no','source','id_name','unnameid','custid','student_feature','trade_no']:\n",
    "        x_columns.append(column_headers[i])  \n",
    "        \n",
    "X = data[x_columns]  # 获取x变量\n",
    "Y = data['status']   # 获取y标签(label)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X, Y)\n",
    "\n",
    "importance = clf.feature_importances_\n",
    "indices = np.argsort(importance)[::-1]\n",
    "features = X.columns\n",
    "for f in range(X.shape[1]):\n",
    "    print((\"%2d) %-*s %f\" % (f + 1, 30, features[f], importance[indices[f]])))\n",
    "    \n",
    "# 特征工程：\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X_fsvar = VarianceThreshold().fit_transform(X) # 过滤掉方差为零的列，并实例化，不填写参数默认方差为0\n",
    "nan_rate = pd.DataFrame((data.shape[0]-data.count())/data.shape[0])  #查看缺失值比例\n",
    "nan_rate\n",
    "\n",
    "# 特征工程：互信息法,它返回每个特征与目标之间的互信息量的估计，这个估计在[0,1]之间取值，0表示独立，1表示两个变量完全相关\n",
    "from sklearn.feature_selection import mutual_info_classif as MIC\n",
    "result = MIC(X,Y)\n",
    "print((result == 0).sum())\n",
    "delete = []\n",
    "for i in range(79):\n",
    "    if result[i] == 0:\n",
    "        delete.append(i)\n",
    "X_ = X.drop(X.iloc[:,delete],axis=1)\n",
    "X_.shape\n",
    "\n",
    "# SMOTE:样本不均衡\n",
    "print(Y[Y==1].count()/Y[Y==0].count())\n",
    "\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=420)\n",
    "X,Y = sm.fit_sample(X_,Y)\n",
    "\n",
    "n_sample_ = X.shape[0]\n",
    "\n",
    "pd.Series(Y).value_counts()\n",
    "\n",
    "n_1_sample = pd.Series(Y).value_counts()[1]\n",
    "n_0_sample = pd.Series(Y).value_counts()[0]\n",
    "print('样本个数：{}; 1占{:.2%}; 0占{:.2%}'.format(n_sample_,n_1_sample/n_sample_,n_0_sample/n_sample_))\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率:0.8522,精确率:0.8758,召回率:0.8213,f1-score:0.8477,auc:0.8522\n"
     ]
    }
   ],
   "source": [
    "# 1,XGBoost:\n",
    "from xgboost.sklearn import XGBClassifier as XGBC\n",
    "clf = XGBC().fit(x_train,y_train)\n",
    "clf.score(x_test,y_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "y_test_pred = clf.predict(x_test)\n",
    "acc = metrics.accuracy_score(y_test,y_test_pred)\n",
    "precision = metrics.precision_score(y_test,y_test_pred)\n",
    "recall = metrics.recall_score(y_test,y_test_pred)\n",
    "f1 = metrics.f1_score(y_test,y_test_pred)\n",
    "auc= metrics.roc_auc_score(y_test,y_test_pred)\n",
    "print('准确率:{:.4f},精确率:{:.4f},召回率:{:.4f},f1-score:{:.4f},auc:{:.4f}'.format(acc,precision,recall,f1,auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_estimators': 100}, 0.932822172461311)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2,网格搜索法进行参数调参： \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_test = {'n_estimators':range(20,200,20)}\n",
    "gsearch = GridSearchCV(estimator = XGBC(learning_rate =0.1, n_estimators=140, max_depth=5, \n",
    "                                                  min_child_weight=1, gamma=0, subsample=0.8, \n",
    "                                                  colsample_bytree=0.8, objective= 'binary:logistic', \n",
    "                                                  nthread=4,scale_pos_weight=1, seed=27), \n",
    "                        param_grid = param_test, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "gsearch.fit(x_train, y_train)\n",
    "# gsearch.grid_scores_, \n",
    "gsearch.best_params_, gsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference：https://github.com/dorisjia/Datawhale/blob/master/task5.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
